)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Training data") %>%
kable_classic(full_width = F, html_font = "Cambria")
library(dplyr)
library(kableExtra)
library(stats)
testData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Test data") %>%
kable_classic(full_width = F, html_font = "Cambria")
str(testData)
levels(trainingData$y)
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
library("caret")
# set seed for repeating the experiments
set.seed(123)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary
)
svm.mod1 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "svmLinear",
trControl = fitControl,
metric = "F"
)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Training data") %>%
kable_classic(full_width = F, html_font = "Cambria")
library(dplyr)
library(kableExtra)
library(stats)
testData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Test data") %>%
kable_classic(full_width = F, html_font = "Cambria")
str(testData)
levels(trainingData$y)
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
library("caret")
# set seed for repeating the experiments
set.seed(123)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary
)
svm.mod1 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "svmLinear",
trControl = fitControl,
metric = "F"
)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Training data") %>%
kable_classic(full_width = F, html_font = "Cambria")
library(dplyr)
library(kableExtra)
library(stats)
testData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Test data") %>%
kable_classic(full_width = F, html_font = "Cambria")
str(testData)
levels(trainingData$y)
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
library("caret")
# set seed for repeating the experiments
set.seed(123)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary
)
svm.mod1 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "svmLinear",
trControl = fitControl,
metric = "F"
)
svm.mod1
y.svm.pred = predict(svm.mod1, testData)
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
#Variable importance by SVM model
varImp(svm.mod1)
svm.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "svmLinear",
trControl = fitControl,
tuneGrid = expand.grid(C = seq(0, 2, length = 20)),
metric = "F"
)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Training data") %>%
kable_classic(full_width = F, html_font = "Cambria")
library(dplyr)
library(kableExtra)
library(stats)
testData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Test data") %>%
kable_classic(full_width = F, html_font = "Cambria")
str(testData)
levels(trainingData$y)
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
library("caret")
# set seed for repeating the experiments
set.seed(123)
library("caret")
# set seed for repeating the experiments
set.seed(123)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE # to visualize running process
)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE # to visualize running process
)
grid_nn <- expand.grid(
size = c(5, 10, 15, 20), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
nn.mod1 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE
)
nn.mod1
plot(nn.mod1)
nn.mod1$bestTune
y.svm.pred = predict(nn.mod1, testData)
y.nn.pred = predict(nn.mod1, testData)
confusionMatrix(y.nn.pred, testData$y, mode = "prec_recall", positive = "yes")
#Variable importance by SVM model
varImp(nn.mod1)
nn.mod1 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000
)
nn.mod1
plot(nn.mod1)
nn.mod1$bestTune
y.nn.pred = predict(nn.mod1, testData)
confusionMatrix(y.nn.pred, testData$y, mode = "prec_recall", positive = "yes")
#Variable importance by SVM model
varImp(nn.mod1)
model_weights <- ifelse(trainingData$y == "no",
(1/table(trainingData$y)[2]) * 0.5,
(1/table(trainingData$y)[1]) * 0.5)
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
weights = model_weights, # apply weights
)
nn.mod2
grid_nn <- expand.grid(
size = c(5, 10, 12), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
weights = model_weights, # apply weights
)
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 50,
weights = model_weights, # apply weights
)
model_weights <- ifelse(trainingData$y == "no",
(1/table(trainingData$y)[2]) * 0.5,
(1/table(trainingData$y)[1]) * 0.5)
grid_nn <- expand.grid(
size = c(2, 5, 10), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 50,
weights = model_weights # apply weights
)
grid_nn <- expand.grid(
size = c(2, 5, 10), # neurons in the hidden layer
decay = c(0.0001)) # decay rate
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 50,
weights = model_weights # apply weights
)
nn.mod2
nn.mod2
plot(nn.mod2)
nn.mod1$bestTune
nn.mod1$bestTune
nn.mod2$bestTune
y.nn.pred = predict(nn.mod1, testData)
y.nn.pred = predict(nn.mod2, testData)
confusionMatrix(y.nn.pred, testData$y, mode = "prec_recall", positive = "yes")
grid_nn <- expand.grid(
size = c(2, 5, 10, 12), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
grid_nn <- expand.grid(
size = c(2, 5, 10, 12), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
nn.mod2 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
weights = model_weights # apply weights
)
nn.mod2
plot(nn.mod2)
nn.mod2$bestTune
y.nn.pred = predict(nn.mod2, testData)
confusionMatrix(y.nn.pred, testData$y, mode = "prec_recall", positive = "yes")
#Variable importance by SVM model
varImp(nn.mod2)
fitControl$sampling <- "down"
fitControl$sampling <- "down"
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
#weights = model_weights # apply weights
)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
samppling = "down"
)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
sampling = "down"
)
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
grid_nn <- expand.grid(
size = c(2, 5, 10, 12), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
sampling = "down"
)
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
grid_nn <- expand.grid(
size = c(5, 10, 12), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
sampling = "down"
)
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
grid_nn <- expand.grid(
size = c(5, 7, 10, ), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
grid_nn <- expand.grid(
size = c(5, 7, 10), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
rm(list = ls())
load("data.RData")
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
sampling = "down"
)
grid_nn <- expand.grid(
size = c(5, 7, 10), # neurons in the hidden layer
decay = c( 0.001, 0.01, 0.1)) # decay rate
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Training data") %>%
kable_classic(full_width = F, html_font = "Cambria")
library(dplyr)
library(kableExtra)
library(stats)
testData %>%
summary(.) %>%
kbl(caption = "Basic statistics. Test data") %>%
kable_classic(full_width = F, html_font = "Cambria")
str(testData)
levels(trainingData$y)
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
library("caret")
# set seed for repeating the experiments
set.seed(123)
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE, # to visualize running process
sampling = "down"
)
grid_nn <- expand.grid(
size = c(5, 7, 10), # neurons in the hidden layer
decay = c( 0.001, 0.01, 0.1)) # decay rate
nn.mod3 <- train(y ~ .,
data = trainingData, # train data
preProcess = c("center","scale"),
method = "nnet",
trControl = fitControl,
tuneGrid = grid_nn,
metric = "F",
verbose = FALSE,
maxit = 5000,
)
grid_nn <- expand.grid(
layer1 = c(2, 5, 10), # neurons in the hidden layer
layer2 = c(2, 5, 10), # neurons in the hidden layer
layer3 = c(2, 5, 10), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
grid_nn <- expand.grid(
layer1 = c(2, 5, 10), # neurons in the hidden layer
layer2 = c(2, 5, 10), # neurons in the hidden layer
layer3 = c(2, 5, 10), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
fitControl <- trainControl(method = "cv",
number = 10,
# Estimate class probabilities
classProbs = TRUE,
summaryFunction = prSummary,
verboseIter = TRUE # to visualize running process
)
grid_nn <- expand.grid(
layer1 = c(2, 5, 10), # neurons in the hidden layer
layer2 = c(2, 5, 10), # neurons in the hidden layer
layer3 = c(2, 5, 10), # neurons in the hidden layer
decay = c(0.0001, 0.001, 0.01, 0.1)) # decay rate
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
rm(list = ls())
load("data.RData")
