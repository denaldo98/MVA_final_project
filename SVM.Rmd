---
title: "Marketing Campaigns of Portuguese Bank"
author: "Denaldo Lapi, Francesco Aristei, Samy Chouiti"
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 10 pt
geometry: margin=0.5in
output:
  pdf_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
  html_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
subtitle: SVM classifier
toc-title: Outline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

Delete all the possible objects of R that could have been left in memory:
```{r, include=TRUE}
rm(list = ls())
```

### Load data

```{r}
load("data.RData")
```

We can change values of the target variable int 'yes' and 'no':

```{r}
testData$y <- as.factor(ifelse(testData$y=='1',"yes","no"))
trainingData$y <- as.factor(ifelse(trainingData$y=='1',"yes","no"))
```

Let's visualize the main statistics of the training data:

```{r}
library(dplyr)
library(kableExtra)
library(stats)
trainingData %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Training data") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Test data:

```{r}
library(dplyr)
library(kableExtra)
library(stats)
testData %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Test data") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Variables structure:

```{r}
str(testData)
```

Levels of the target factor variable:

```{r}
levels(trainingData$y)
```

We can change the levels ordering:

```{r}
trainingData$y <- relevel(trainingData$y, ref = "yes")
testData$y <- relevel(testData$y, ref = "yes")
levels(trainingData$y)
```

The positive value is now located on position 1 of the levels.


# SVM classifier

We' ll use the *caret* package for applying SVM, which performs a non-linear classification using what is called the kernel trick. The most commonly used kernel transformations are polynomial kernel and radial kernel.

At first let's load the *caret* package:

```{r}
library("caret")
```

```{r}
# set seed for repeating the experiments
set.seed(123)
```


We'll run SVM on the training dataset and applying cross-validation (10-fold):

```{r}
fitControl <- trainControl(method = "cv",
                           number = 10, 
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           summaryFunction = prSummary
)
```


## SVM with linear kernel

Let's first run the SVM with a linear kernel, which is a simple linear classifier. For the moment we'll not apply any kind of hyperparameter tuning.
We will this first SVM model as a baseline for building the following ones. 
Again we'll center and scale the variables to make their scale comparable, and we'll use the _F-score_ as a validation metric:

```{r}
svm.mod1 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmLinear", 
                 trControl = fitControl,
                 metric = "F"
)
```

Let's print the results:

```{r}
svm.mod1
```
As shown by the result, the tuning parameter 'C' is set to a standard value of 1 (no tuning). 
This parameter imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.


Predict test data:

```{r}
y.svm.pred = predict(svm.mod1, testData)
```

We can show the confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

This model has lower performances than our best KNN model.


We can visualize the importance of each variable:

```{r}
#Variable importance by SVM model
varImp(svm.mod1)
```

We can now perform hyperparameter tuning of the 'C' parameter: we can create a grid with different values of it:


```{r}
svm.mod2 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmLinear", 
                 trControl = fitControl,
                 tuneGrid = expand.grid(C = seq(0, 2, length = 20)),
                 metric = "F"
)
```

```{r}
svm.mod2
```

Plot F-score with different values of cost:

```{r}
plot(svm.mod2)
```


Print the best tuning parameter C that maximizes model F-score:

```{r}
svm.mod2$bestTune
```


Predict test data:

```{r}
y.svm.pred = predict(svm.mod2, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

As we can see, parameter C doesn't change the performances.

Variable importance

```{r}
#Variable importance by SVM model
varImp(svm.mod2)
```


## SVM with Radial Kernel

*Caret* package allows to use a radial kernel function, that allows to find a non-linear classification boundary among the 2 classes.
In this case we'll use the 'tuneLength' parameter wich randomly tries several parameters combinations.

```{r}
svm.mod3 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 20,
                 metric = "F"
)
```

```{r}
svm.mod3
```

Plot F-score with different values of cost:

```{r}
plot(svm.mod3)
```


Print the best tuning parametes  that maximizes model F-score:

```{r}
svm.mod3$bestTune
```


Predict test data:

```{r}
y.svm.pred = predict(svm.mod3, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

We've slightly improved the F-score, reaching a value of 0.36.

Variable importance

```{r}
#Variable importance by SVM model
varImp(svm.mod3)
```

## SVM with Polynomial Kernel

*Caret* package allows also to use a polynomial kernel function, that allows to find a non-linear classification boundary among the 2 classes.
We'll use the 'tuneLength' parameter which randomly tries several parameters combinations.

```{r}
svm.mod4 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmPoly", 
                 trControl = fitControl,
                 tuneLength = 3,
                 metric = "F"
)
```

```{r}
svm.mod4
```

Plot F-score with different values of cost:

```{r}
plot(svm.mod4)
```


Print the best tuning parameters  that maximizes model F-score:

```{r}
svm.mod4$bestTune
```


Predict test data:

```{r}
y.svm.pred = predict(svm.mod4, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

As we can see, parameter C doesn't change the performances.

Variable importance

```{r}
#Variable importance by SVM model
varImp(svm.mod4)
```


# Handling class imbalance

Since the problem we are facing is very challenging due to the fact that one class heavily outweights the other, we'll try to adopt some strategies to reduce the effect of this class imbalance on the training of the algorithm.

As a first solution, we'll change the metric used to optimize the model from the F-score to the ROC. 
Indeed the F-score is computed on the confusion matrix, therefore it assumes an hard cutoff on the preficted probabilities: this means that they use a standard cutoff of 0.5 to predict a sample. This results in the model predicting that all observations belong to the majority class.

On the contrary, the ROC metric is threshold-invariant: it quantifies true positive rate as a function of false positive rate for a variety of classification thresholds. Another way to interpret this metric is the probability that a random positive instance will have a higher estimated probability than a random negative instance.


## SVM with ROC metric

We change the 'summaryFunction' parameter in the 'trainControl' method:

```{r}
fitControl <- trainControl(method = "cv",
                           number = 10, 
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary
)
```


Let's run the model usign the ROC:

```{r}
svm.mod5 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 10,
                 metric = "ROC"
)
```


```{r}
svm.mod5
```

Plot F-score with different values of cost:

```{r}
plot(svm.mod5)
```


Print the best tuning parameters  that maximizes model F-score:

```{r}
svm.mod5$bestTune
```


Predict test data:

```{r}
y.svm.pred = predict(svm.mod5, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```


Variables importance:

```{r}
#Variable importance by SVM model
varImp(svm.mod5)
```




We now build a function to plot the AUC (area under the curve), that represents the area under the ROC curve. This function takes the model and the test set and returns the AUC value:


```{r}
test_roc <- function(model, data) {
  
  roc(data$y,
      predict(model, data, type = "prob")[, "yes"])
}
```

```{r}
#install.packages("pROC")
library(pROC)

svm.mod5 %>%
  test_roc(data = testData) %>%
  auc()
```

CALCOLARE LA ROC NEL SVM CON F SCORE


We'll now try to improve the ROC by using the under-sampling technique

### Down-sampling

Down-sampling method consists in removing instances in the majority class.
In can be easily implemented by using the sampling argument in the trainControl function.

We add the sampling parameter to the 'fitControl' function:

```{r}
fitControl$sampling <- "down"
```


Fit the model:

```{r}
svm.mod6 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 10,
                 metric = "ROC",
                 verbose = FALSE
)
```

```{r}
svm.mod6
```

```{r}
svm.mod6 %>%
  test_roc(data = testData) %>%
  auc()
```
We've improved the AUC metric to 89.34%

Predict test data:

```{r}
y.svm.pred = predict(svm.mod6, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

We've achieved a very nice result and improvement with respect to all the previously analyzed models:
* F1 score is 48%
* Balanced Accuracy is 79.8%


### Smote

Another possible approach is to apply the so-called Synthetic minority sampling technique (SMOTE), which down samples the majority class and synthesizes new minority instances by interpolating between existing ones.


We add the sampling parameter to the 'fitControl' function:

```{r}
fitControl$sampling <- "smote"
fitControl$verboseIter = TRUE # to visualize running process
```


Fit the model:

```{r}
svm.mod7 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 10,
                 metric = "ROC",
                 verbose = FALSE
)
```


```{r}
svm.mod7 %>%
  test_roc(data = testData) %>%
  auc()
```



Predict test data:

```{r}
y.svm.pred = predict(svm.mod7, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```

This model has quite similar results to the previous one:
* it slighlty reduces the AUC and the balanced accurcay
* but it improves the F1 score






As a further step, we could also try the Up-sampling technique, which consists in randomly replicating instances in the minority class. However, due to the high size of the training dataset, the training process and the cross-validation may require a lot of time, therefore we'll now focus of doing som more hyperparameter tuning on the best SVM model found so far, i.e. the one based on under-sampling.


### More hyperparameter tuning

We first change the 'sampling' parameter:

```{r}
fitControl$sampling <- "down"
```


We now define a list of tuning parameters 'C' and 'sigma':

```{r}
grid_radial <- expand.grid(
  sigma = c(0,0.01, 0.02, 0.025, 0.03, 0.04, 0.05, 0.06, 0.07,0.08, 0.09, 0.1,   0.14, 0.25, 0.5,0.75,0.9),
  C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2,5))
```

We can now train the model:

```{r}
svm.mod8 <- train(y ~ ., 
                 data = trainingData, # train data  
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneGrid = grid_radial,
                 tuneLength = 10,
                 metric = "ROC",
                 verbose = FALSE
)
```


```{r}
svm.mod8 %>%
  test_roc(data = testData) %>%
  auc()
```
We've improved the AUC metric to 89.34%

Predict test data:

```{r}
y.svm.pred = predict(svm.mod8, testData)
```

Confusion matrix:

```{r}
confusionMatrix(y.svm.pred, testData$y, mode = "prec_recall", positive = "yes")
```



























### Weighting 

We'll now use class weights to impose a heavier cost when errors are made in the minority class.

Let's now create model weights:
* for 'no' examples we give a lower weight
* for 'yes' examples we give an higher weight

Notice that the sum of the weights is 1:

```{r}
model_weights <- ifelse(trainingData$y == "no",
                        (1/table(trainingData$y)[2]) * 0.5,
                        (1/table(trainingData$y)[1]) * 0.5)
```


We can now run fit the model by using the new weights:

```{r}
svm.mod6 <- train(y ~ ., 
                 data = trainingData,
                 preProcess = c("center","scale"),
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 10,
                 metric = "ROC",
                 weights = model_weights, # apply weights
                 verbose = FALSE
)
```


```{r}
svm.mod6
```

```{r}
svm.mod6 %>%
  test_roc(data = testData) %>%
  auc()
```



