---
title: "Marketing Campaigns of Portuguese Bank"
author: "Denaldo Lapi, Francesco Aristei, Samy Chouiti"
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 10 pt
geometry: margin=0.5in
output:
  pdf_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
  html_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
subtitle: Predict client subscription
toc-title: Outline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

Delete all the possible objects of R that could have been left in memory:
```{r, include=TRUE}
rm(list = ls())
```

### Load packages
At first, let's load the libraries we'll need:

* *dplyr* and *ggplot2* will help us with data manipulation and graphs, respectively.
* *kableExtra* will help us to print beautiful tables.
* *gridExtra* for arranging the layout of the graphs.
* *stats* computes hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it ( _hclust_).
* *cluster* to apply the _agnes_ (agglomerative hierarchical clustering) and the _diana_ (divisive hierarchical clustering).
* *gplots* computes heatmaps for visualizing the distance matrix.
* *factoextra* for MVA methods and graph the clustering structures.
* *FactoMineR* computes hierarchical clustering on principal components.

```{r message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(stats)
library(gplots)
library(cluster)
library(factoextra)
library(FactoMineR)
```



# Exploratory data analysis

The dataset we are going to use....
A bried description of each variable:
*
*
*

At first let's load the data: 
```{r}
data = read.csv("bank.csv", sep = ";", header = TRUE)
```

```{r}
head(data)
```

Since the data matrix contains one observation for each customer, we identify each individual with a progressive integer ID:

```{r}
data$ID <- seq.int(nrow(data))
```

```{r}
head(data)
```

We can reorder columns so to have the id column as the first one:

```{r}
data <- data %>%
  select(ID, everything())
```

```{r}
head(data)
```


Check dimension:

```{r}
dim(data)
```

The dataset was correctly read: it has 4521 rows/observations and 18 columns/variables

Check the structure:

```{r}
str(data)
```
As we can see, all categorical variables has been read as 'character' by R, so let's transform them into the 'factor' R data type:

```{r}
data[,"job"] = as.factor(data[,"job"])
data[,"marital"] = as.factor(data[,"marital"])
data[,"education"] = as.factor(data[,"education"])
data[,"default"] = as.factor(data[,"default"])
data[,"housing"] = as.factor(data[,"housing"])
data[,"loan"] = as.factor(data[,"loan"])
data[,"contact"] = as.factor(data[,"contact"])
data[,"month"] = as.factor(data[,"month"])
data[,"poutcome"] = as.factor(data[,"poutcome"])
data[,"y"] = as.factor(data[,"y"])
```

We can check again the structure:

```{r}
str(data)
```

As we can see, the categorical variables have been all correctly converted into the 'factor' type.

We can print a portion (a sample of 20) of the table using kable and the pipe operator, to better understand the structure of the variables:

```{r}
data %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Marketing Campaigns of Portuguese Bank (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Let's now visualize some basic statistics on each of the data frame's columns with *summary*:

```{r}
data %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Marketing Campaigns of Portuguese Bank") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
The previous output allows to understand some important properties of the dataset. Firstly we can see the possible values assumed by each categorical variable.

Some relevant observations that can be drawn are:
* the dataset represent customers with a medium age of 41 years, which a pretty realistic scenario in a bank database
* the 'marital' variable shows that the main customers of the bank are families, since most of the 'marital' variable values are in the 'married' category
* the variable 'balance' assumes also negative values, indicating people with a negative balanced in their bank account
* variables default is highly unbalanced towards the value 'yes', meaning that very few customers have their credit in default
* the variable 'loan' tells us that most of the bank clients don't have any kind of personal loan
* the variable 'previous' is characterized by a very low mean value, meaning that the majority of the clients have been contacted only a few times before this specific campaign
* a value of '-1' in the 'pdays' variable represents clients who have never been contacted before for marketing campaign
* most of the values of the variable 'poutcome' are 'unknown' meaning that the bank doesn't have any data regarding the outcome of previous campaigns for that client. It could be interesting to understand whether the 'unknown' values are associated only to '-1' values of the 'pdays' variable: if it is the case it means that the bank has no information only about customers who have never been contacted before for a campaign.
* for what regard the target variable 'y', we can see how most of the observations belong to the 'no' class, with only 521 observations belonging to the class 'yes'. This is an important characteristic of out dataset, that we should strongly take into consideration when developing our classification models, in order to avoid giving to much importance to the 'no' class. More observation about this unbalance problem will be discussed in the following sections.

### Check for missing values

Let's check for NA values for each column:

```{r}
colSums((is.na((data))))
```

There are no missing values!


### Check variables distribution

Before dealing with outliers, we would like to better inspect the data at our disposal by directly visualizing the distribution of each variable. This will allow us to understand the characteristics of the variables and to decide the strategy to adopt for dealing with eventual outliers.

At first, let's visualize the distribution of each qualitative variable, by using bar plots.

'job' variable:

```{r message=FALSE,warning=FALSE}

p <- ggplot(data, aes(x=job, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "job variable",
    x = "job",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```

'marital' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=marital, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "marital variable",
    x = "marital",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```


'education' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=job, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "education variable",
    x = "education",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```
'default' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=default, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "default variable",
    x = "default",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

'housing' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=housing, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "housing variable",
    x = "housing",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

Observations:
* it can be seen that people with no housing loan are more prone to subscribe for a term deposit with respect to people with a housing loan


'loan' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=loan, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "loan variable",
    x = "loan",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```

'contact' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=contact, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "contact variable",
    x = "contact",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

'month' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=month, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "month variable",
    x = "month",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

Observations:
* most of the last contacts have happened in the month of may
* people contacted in October have an higher possibility of signing for a term deposit 


'poutcome' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=poutcome, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "poutcome variable",
    x = "poutcome",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

Observations:
* as expected, people who decided to sign for a term deposit in a previous campaign are very prone to re-sign for the current campaign
* most of the values are 'unknown': 


There is some ambiguity on the meaning of 'other' and 'unknown': we may decide to convert into 'unknown' the value of the observations with 'poutcome' set to 'other': 

```{r}
data$poutcome[data$poutcome=="other"] <- "unknown"
```



```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=poutcome, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "poutcome variable",
    x = "poutcome",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

We can now visualize more in detail the distribution and characteristics of the quantitative variables of our dataset.
Let's start by analyzing the distributions:

```{r}
g1 <- ggplot(data, aes(x=age)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g2 <- ggplot(data, aes(x=balance)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g3 <- ggplot(data, aes(x=day)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g4 <- ggplot(data, aes(x=duration)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g5 <- ggplot(data, aes(x=campaign)) +
  geom_histogram(alpha=0.6, fill="#69b3a2", color="#e9ecef", alpha=0.8)
g6 <- ggplot(data, aes(x=pdays)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g7 <- ggplot(data, aes(x=previous)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)

grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7, nrow=1);
```






We can visualize also densities depending on the target variable value:

```{r}
g1 <- ggplot(data, aes(x=age, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="right")
g2 <- ggplot(data, aes(x=balance, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g3 <- ggplot(data, aes(x=day, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g4 <- ggplot(data, aes(x=duration, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g5 <- ggplot(data, aes(x=campaign, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g6 <- ggplot(data, aes(x=pdays, fill=y)) +
   geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g7 <- ggplot(data, aes(x=previous, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")

grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7, nrow=1);
```

Observations:
* 'duration' variable has a pretty accentuated role in discrimating between the 2 classes of the target variable
* variable 'day' has a pretty uniform distribution with a peak aroun the middle of the month (both for 'yes' and 'no' classes)
* 'campaign' distribution is carachterzied for some small peaks for high values of the variable in corresponding to the class 'no', meaning that the more the client is called, the less it's probable that he will be likely to sign for the term deposit



We can now get a rough estimate of the distribution of the values for each continuous attribute broken down by each class:

```{r}
library(gridExtra)
g1 <- ggplot(data,aes(x=y, y=age, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="right")
g2 <- ggplot(data,aes(x=y, y=balance, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g3 <- ggplot(data,aes(x=y, y=day, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g4 <- ggplot(data,aes(x=y, y=duration, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g5 <- ggplot(data,aes(x=y, y=campaign, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g6 <- ggplot(data,aes(x=y, y=pdays, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g7 <- ggplot(data,aes(x=y, y=previous, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
grid.arrange(g1,g2,nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7,nrow=1);

```
Observations:
* for the 'balance' variable we can say that we have a very sparse solution: most of the clients have a very low bank account balance, while we have 2 observations with negative value and other 2 observations with a very high balance value.
* for what regards the 'duration' variable we have 3 samples with very high values of the variable. The distribution of the variable is left skewed, with a very low mean, and there are a lot of values above the interquantile range, which we consider significant for the classification task and we will not consider all of them as outliers.
* also in 'campaign' we have some observations with too high value: it seems that there are clients that have been contacted more than 40 times, with respectr to an average of arounf 2.8.

We think it is interesting to see the results for the clients with the highest values of 'balance':

```{r}
arr = c(order(data$balance, decreasing=TRUE)[1:10])
data[arr,]
```
We can see that no one of these clients decided to sign for the term deposit.

We can use the *boxplot* function for selecting the outliers:

```{r}
#grab the outliers
outliers = boxplot(data$balance, plot=FALSE)$out

#Extract the outliers from the original data frame
data[data$balance %in% outliers,]
```

The boxplot function selects 506 observations as outliers!
We' ll remove only the first 2 observations with the highest balance:

```{r}
arr = c(order(data$balance, decreasing=TRUE)[1:2])
data = data[-arr, ]
```

Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=balance, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=balance)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```



We think also that the variable 'campaign' may be pretty indicator for predicting the our target variable, since it represents the number of times a client has been contacted during the actual marketing campaign:


```{r}
arr = c(order(data$campaign, decreasing=TRUE)[1:10])
data[arr,]
```
`

What we can understand from the above is that  calling many times a client has not a positive effect on their willingness to sign for the term deposit.


We' ll remove only the first 2 observations with the highest campaign, since they consider clients called more that 40 times, which is a really high value that cannot be considered much realistic, especially considering the mean of that variable (2.8):

```{r}
arr = c(order(data$campaign, decreasing=TRUE)[1:2])
data = data[-arr, ]
```

Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=campaign, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=campaign)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

The same reasoning can be applied for the 'duration' variable:
```{r}
arr = c(order(data$duration, decreasing=TRUE)[1:20])
data[arr,]
```

We think this variable is crucial for discriminating between the 2 classes: indeed an higher call duration indicates an higher probability for the clien to apply for the term deposit.
We'll remove the first 3 observations with highest value of 'duration':

```{r}
arr = c(order(data$duration, decreasing=TRUE)[1:3])
data = data[-arr, ]
```


Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=duration, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=duration)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```



In could be interesting to visualize the correlation among these continuous variables:

```{r}
cols = c("age", "balance", "day", "duration", "campaign", "pdays", "previous")
library(GGally)
ggpairs(data, columns=cols,
        ggplot2::aes(colour=y),
        title="Correlation matrix")
```

We have very low correlation values, except for the pair of variables 'pdays' and 'previous'.



## Insights about the data

With the obtained knowledge of the distribution of the variables, we can perform further analysis of each customer characteristic in order to investigate its influence on the subscription rate, i.e. on the rate of subscriptions to the term deposit.

Let's better visualize the scatter plot of the 2 variables 'campaign' and 'duration', encoding the target class in the plot:

```{r}
ggplot(data, aes(x=duration, y=campaign, shape=y, color=y, size=y)) +
  geom_point()
```

As shown in the above plot, 'duration' and 'campaign' variables allow to create approximately 2 clusters of customers:
* almost all the 'yes' clients are characterized by low value of 'campaign' and they may have also high 'duration'
* 'no' clients have higher values on campaign and low 'duration' values

An important observation is that an high value of the 'campaign' variable makes the clients to refuse to sign for the term deposit: almost all observations with 'campaign' value above 10 have 'no' as target variable value.



It may be useful to better visualize the scatter plot of the 2 variables 'age' and 'contact', because it is more probable that older people are more used to be contacted by telephone:

```{r}
ggplot(data, aes(x=contact, y=age, shape=y, color=y, size=y)) +
  geom_point()
```

It seems the there is no relationship between 'contact' and 'age'.


It could be interesting to better visualize the scatter plot of the 2 variables 'age' and 'balance', encoding the target class in the plot:

```{r}
ggplot(data, aes(x=age, y=balance)) +
  geom_point()
```
The above scatter plot doesn't show any strong relationship among the 2 variables: most of the customers contacted by the bank has low salary, independently from the age.


We can now represent the correlation matrix among these quantitative variables: 
```{r}
# compute correlation matrix
res <- cor(data[, cols])
round(res, 2)
```

```{r}
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
The results confirms what we observed before: only 'pdays' and 'previous' are positively correlated to each other.



SUBSCRIPTION RATE PER AGE
```{r}
# convert age variable into bins
#Coverting pdays variable into bins
data2 = data

data2$age = cut(data2$age, breaks = c(0, 29, 39, 49, 59, 69, 100),
                 labels = c('<30','30-39','40-49',
                            '50-59','60-69','70+'
                            ))
```

```{r}
data$age[16]
```


```{r}
data2 %>% 
    count(age = factor(age), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```

The above plot suggests that:
* clients with a age above 60 have the highest subscription rate: since the percentage of 'yes' is very close to that of 'no', meaning that a lot of the people contacted by the bank signed for the term deposit

* it is not surprising to see such a pattern because the main investment objective of older people is saving for retirement while the middle-aged group tend to be more aggressive with a main objective of generating high investment income. Term deposits, as the least risky investment tool, are more preferable to the eldest.
* The youngest may not have enough money or professional knowledge to engage in sophisticated investments, such as stocks and mutual funds. Term deposits provide liquidity and generate interest incomes that are higher than the regular saving account, so term deposits are ideal investments for students.

We could better visualize this characteristics by taking into account only customers that signed a term deposit:



```{r}
data_yes <- data2[which(data2$y == 'yes'), ]  # all yes's of target class

data_yes %>% 
    count(age = factor(age), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, label = scales::percent(pct))) + 
    geom_col(position = 'dodge', fill="#69b3a2", color="#e9ecef", alpha=0.8) + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```

The plot clearly shows the the age range of people most contacted by the bank is the one between 30 and 60, this explains the high percentage of subscriptions. But this may be misleading, as we saw above that older people are much more likely to sign.


# SUBSCRIPTION RATE PER JOB

```{r}
data %>% 
    count(job = factor(job), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = job, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)+
    theme_grey()+
    coord_flip()
```

The plot above allows to draw other interesting conclusions, also related to what we saw before:
* the percentage of 'yes' is very high for 'retired' and 'student' categories, as we already found above regarding the age ranges


subscription rate by balance level

In order to better understand the effect of the balance on the probability to sign for a term contract, we can visualize it by means of a histogram


```{r}
# cut balances into 
data2 <- data
data2$balance = cut(data2$balance, breaks = c(-5000, -1,  2000, 5000, 10000, 15000, 30000),
                 labels = c('<0', '0-2000', '2000-5000','50000-10000', '10000-15000', '>150000'
                            ))
```


data2$age = cut(data2$age, breaks = c(0, 29, 39, 49, 59, 69, 100),
                 labels = c('<30','30-39','40-49',
                            '50-59','60-69','70+'
                            ))


```{r}
data2 %>% 
    count(balance = factor(balance), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = balance, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```
Observations:
* clients with a balance in the range 2000-5000 are the ones with a highest subscription rate



In order to confirm our hypoteses, we can combine together the information from the ages with that of the balances

```{r}
data2 <- data
data2$balance = cut(data2$balance, breaks = c(-5000, -1,  5000, 15000, 30000),
                 labels = c('<0', '0-5000', '5000-15000','>15000'
                            ))
data2$age = cut(data2$age, breaks = c(0, 29, 39, 49, 59, 69, 100),
                 labels = c('<30','30-39','40-49',
                            '50-59','60-69','70+'
                            ))
data_yes <- data2[which(data2$y == 'yes'), ]  # all yes's of target class
data_yes %>% 
    count(age = factor(age), bal = factor(balance)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, fill = bal, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)+
    coord_flip()
```

We can see that the highest number of 'yes' is in the age range '30-39' for clients with a balance between '0-5000', this means young clients with a sufficient balance are the most prone to sign for the term deposit.


## Preparing the dataset

We'll now prepare the dataset for the classification task. We remind that our goal is to use the customers characteristics to predict whether they will sign for a term deposit. This will allow the bank to identify the customers to contact in future marketing campaigns.

### Feature engineering

We think that the variables 'contact' and 'day' are not very useful for the classification task:

* for what regards the 'day' variable we have seen in the previous plots how it' density is pretty uniform along the days of the month, therefore we'll not consider that variable for performing the classification algorithms

* the variable 'contact' indicates the type of mean used to contact the customers: the above plots show that it is not an interesting feature for the classification task.


```{r}
drop = c("contact", "day")
data = data[,!(names(data) %in% drop)]
```

```{r}
str(data)
```

Since the variable 'pdays' is very spread in the past and has a lot of '-1' values (which makes the classification task harder compromising the training), we decided to group values into 7 bins:
```{r}
data$pdays = cut(data$pdays, breaks = c(-2,-1,90,181,272,363,545,max(data$pdays)+1),
                 labels = c('Never contacted','Under 3months','Under 6months',
                            'Under 9months','Under 1yr','Under 1.5yrs',
                            'Over 1.5yrs'))
summary(data$pdays)
```



### Convert categorical variables

Since most classification algorithm work with continuous variables, we need to use a one-hot encoding of them.
In order to do that, we'll convert all the categorical variables into 'dummy' variables, i.e. we  represent a categorical variable as a numerical variable for each category that takes on one of two values: zero or one

We'll use the _dummyCols_ function of the *fastDummes* package of R:

Let's see how it works for a single variable ('poutcome'):

```{r}
#install.packages('fastDummies')
library('fastDummies')

# Create dummy variables:
data_dummy <- dummy_cols(data, select_columns = 'poutcome', remove_selected_columns = TRUE)
```

```{r}
head(data_dummy)
```

```{r}
summary(data_dummy)
```
As we can see, the creation of the dummy variables created a variable also for the value 'other' of the 'poutcome' variable, which we converted before into the 'unknown' value. Indeed the new variable 'poutcome_other' has value 0 for all individuals, we'll therefore drop it from our dataframe:

```{r}
drop = c("poutcome_other")
data_dummy = data_dummy[,!(names(data_dummy) %in% drop)]
```

```{r}
str(data_dummy)
```

We can now follow the same reasoning for the other categorical variables:

```{r}
data_dummy <- dummy_cols(data_dummy, select_columns = c('job','marital', 'education', 'default', 'housing', 'loan', 'month', 'pdays'), remove_selected_columns = TRUE)
```

Let's visualize a sample of the newly created data matrix:


```{r}
data_dummy %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Marketing Campaigns of Portuguese Bank (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


The last operation we need to do is to convert the 'yes' value of 'y' variable into 1 and 'no' into 0:
```{r}
data_dummy$y <- ifelse(data_dummy$y=="yes",1,0)
```

```{r}
data_dummy %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Marketing Campaigns of Portuguese Bank") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

### Multivariate outliers

Now that we have the explored enough the data and converted it into the final format we'll use in the following, we can perform some multivariate outlier detection algorithms, for detecting and removing outliers in the multivariate space.

One way to check for multivariate outliers  (non-parametric approach) is to use the LOF ("local outlier factor") algorithm, which identifies density-based local outliers.

The algorithm we are going to use (from the package [DDoutlier](https://rdrr.io/cran/DDoutlier/man/LOF.html)) computes a local density for observations with a given k-nearest neighbors (we choose k = 5). This local density is compared to the density of the respective nearest neighbors, resulting in the local outlier factor.

Therefore, the function returns a vector of LOF scores for each observation: the greater the LOF, the greater the outlierness of the data point.


```{r message=FALSE, warning=FALSE}
#install.packages('DDoutlier')
library("DDoutlier")
lof <- LOF(data_dummy, k = 5) # outlier score with a neighborhood of 5 points
```

We can show the lof scores for the 5 first observations:

```{r}
head(lof)
```

We can see and visualize the distribution of outlier scores:

```{r}
summary(lof) # some statistics
hist(lof)
```

It could be useful to plot also the sorted LOF scores:

```{r}
plot(sort(lof), type = "l",  main = "LOF (K = 5)",
  xlab = "Points sorted by LOF", ylab = "LOF")
```

Looks like outliers start around a LOF value of 2.0.

Let's show the indexes for 5 most outlying observations

```{r}
lof_with_names = lof
names(lof_with_names) <- 1:nrow(data_dummy)
sort(lof_with_names, decreasing = TRUE)[1:5]
```

Let's first find the indexes of the outliers with a lof score above 2.0:

```{r}
outliers_lof <- which(lof > 2.0)
```

Number of detected outliers:

```{r}
length(outliers_lof)
```

```{r}
outliers_lof
```


We will simply remove the found outliers (found with lof) from the dataset (taking into account that k-means algorithm is very sensible to outliers):

```{r}
data_dummy = data_dummy[-outliers_lof,] 
```

Let's now check again the dimensions:

```{r}
dim(data_dummy)
```

The outliers have been correctly removed!



### PCA

A further step to gain more insights about the dataset could be to perform dimensional reductions by applying factor analysis methods.

Since we don't have specific groups of variables, we'll perform first a PCA by using the continuous features and by adding the categorical variables as supplementary information:
* our purpose is to find some useful relationships between variables and individuals, with respect to the class
* we will exploit these insights for performing the classification

We'll use *FactoMineR* for performing the PCA:

```{r}
str(data_dummy)
```


```{r}
data_ = data_dummy[2:18]
res.pca = PCA(data_, scale.unit=TRUE, ncp=5, graph=F)
```

```{r}
fviz_eig(res.pca)
```

```{r}
?fviz_pca_ind
```



Individuals plot:

```{r}
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             select.ind = list(contrib =50),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
fviz_pca_ind(res.pca,
             col.ind = data_dummy$y,
             repel = TRUE     # Avoid text overlapping
             )
```


We are more interested in visualizing the variables plot:

```{r}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
The above PCA plot shows an high correlation for the variables 'previous' and 'pdays' as well as for the variables 'campaign' and 'day'. Variable 'duration' is negatively correlated with 'campaign' and 'day'. While variables balance and age are not well represented.
We may perform the plot on the 3rd and 4th PCs:

```{r}
fviz_pca_var(res.pca,
             axes = c(3,4),
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
We can see that on the 2nd factorial plane the variables 'balance' and 'age' are very correlated with each other, and they are very correlated with the 3rd dimension. 
The correlation between 'balance' and 'age' may be easily interpretable with the fact that younger people are unlikely to have high balances in their bank account.

Biplot:

```{r}
data_dummy[,"y"] = as.factor(data_dummy[,"y"])
fviz_pca_biplot(res.pca, repel = TRUE,
                axes = c(1,2),
                col.var = "black", # Variables color
                col.ind = data_dummy$y,
                addEllipses = TRUE
                )
```

Since PCA tries to maximize the variance inside the dataset, it is not so useful for a classification task.

Our next step is therefore to try to apply an LDA, just for visualizing better the data:

```{r}
library(MASS)
(model <- lda(y~., data = data_dummy))
```

```{r}
plot(model)
```



### Splitting

We' ll split the dataset into training and validattion:
* 80% of the original data will be used for training
* the reaming 20% will be used to evaluate the final performance of the classifier

An important remark is that the considered dataset is highly unbalanced: most of the customers belong to the 'no' values of the target variable. For that reason, and in other to have a fair evaluation of our algorithms we'll perform a stratified splitting:
* we'll select 80% among all the clients belonging to the positive class for training and 20% for testing
* we'll select 80% among all the clients belonging to the negative class for training and 20% for testing

In this way we'll have the same distribution of observations inside the two classes in both the training and test set.



```{r}
data_dummy <- data_dummy %>% relocate(y, .after = month_sep )
```

```{r}
str(data_dummy)
```


```{r}
set.seed(123)
#Training & Test Datasets
deposit_yes <- data_dummy[which(data_dummy$y == 1), ]  # all yes's of target class
deposit_no <- data_dummy[which(data_dummy$y ==  0), ]  # all no's of target class

deposit_yes_training_rows <- sample(1:nrow(deposit_yes), 0.8*nrow(deposit_yes))  #randomly choosing 80% observations of yes class
deposit_no_training_rows <- sample(1:nrow(deposit_no), 0.8*nrow(deposit_no))  #randomly choosing 80% observations of no class
training_yes <- deposit_yes[deposit_yes_training_rows, ]  
training_no <- deposit_no[deposit_no_training_rows, ]
trainingData <- rbind(training_yes, training_no)  #combining chosen observations
glimpse(trainingData)
table(trainingData$y)

```



In the training data we have 3185 samples in the 'no' class and 412 samples in the 'yes' class.


```{r}
# Create Test Data
test_yes <- deposit_yes[-deposit_yes_training_rows, ]
test_no <- deposit_no[-deposit_no_training_rows, ]
testData <- rbind(test_yes, test_no)  #combining chosen observations
glimpse(testData)
table(testData$y)
```

In the test data we have 797 samples in the 'no' class and 104 samples in the 'yes' class.




### Scaling

We' ll perform the scaling according to the needs of each different classifier we'll use, since not all algorithms need scaled data.

For the moment we build also a scaled version of the dataset, so that we can re-use it later on:
```{r}
str(data_dummy)
```

```{r}
#Normalizing continuous variables
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
data_dummy_scaled = data_dummy
data_dummy_scaled$age = normalize(data_dummy_scaled$age)
data_dummy_scaled$balance = normalize(data_dummy_scaled$balance)
data_dummy_scaled$campaign = normalize(data_dummy_scaled$campaign)
data_dummy_scaled$duration = normalize(data_dummy_scaled$duration)
data_dummy_scaled$previous = normalize(data_dummy_scaled$previous)
```







```{r}
data_dummy_scaled[,"y"] = as.factor(data_dummy_scaled[,"y"])
str(data_dummy_scaled)
```

```{r}
summary(data_dummy_scaled)
```


We can now re-preform the splitting on the scaled data:

```{r}
deposit_yes <- data_dummy_scaled[which(data_dummy_scaled$y == 1), ]  # all yes's of target class
deposit_no <- data_dummy_scaled[which(data_dummy_scaled$y ==  0), ]  # all no's of target class

training_yes <- deposit_yes[deposit_yes_training_rows, ]  
training_no <- deposit_no[deposit_no_training_rows, ]
trainingData_scaled <- rbind(training_yes, training_no)  #combining chosen observations
glimpse(trainingData_scaled)
table(trainingData_scaled$y)
```

```{r}
# Create Test Data
test_yes <- deposit_yes[-deposit_yes_training_rows, ]
test_no <- deposit_no[-deposit_no_training_rows, ]
testData_scaled <- rbind(test_yes, test_no)  #combining chosen observations
glimpse(testData_scaled)
table(testData_scaled$y)
```

We can save the scaled and the not scaled training and test dataset:

```{r}
save(trainingData, testData, trainingData_scaled, testData_scaled, file = "data.RData")
```



