---
title: "Marketing Campaigns of Portuguese Bank"
author: "Denaldo Lapi, Francesco Aristei, Samy Chouiti"
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 10 pt
geometry: margin=0.5in
output:
  html_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
  pdf_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: True
    df_print: kable
subtitle: Predict client subscription
toc-title: Outline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

Delete all the possible objects of R that could have been left in memory:

```{r, include=TRUE}
rm(list = ls())
```

### Load packages
At first, let's load the libraries we'll need:

* *dplyr* and *ggplot2* will help us with data manipulation and graphs, respectively.

* *kableExtra* will help us to print beautiful tables.

* *gridExtra* for arranging the layout of the graphs.

* *stats* computes hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it ( _hclust_).

* *cluster* to apply the _agnes_ (agglomerative hierarchical clustering) and the _diana_ (divisive hierarchical clustering).

* *gplots* computes heatmaps for visualizing the distance matrix.

* *factoextra* for MVA methods and graph the clustering structures.

* *FactoMineR* computes hierarchical clustering on principal components.

```{r message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(stats)
library(gplots)
library(cluster)
library(factoextra)
library(FactoMineR)
```


# Exploratory data analysis

The dataset we are going to use is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).
A brief description of each variable:

# bank client data:

* age (numeric)

* job : type of job (categorical:
'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')

* marital : marital status (categorical: 'divorced','married','single','unknown')

* education (categorical)

* default: has credit in default? (categorical: 'no','yes','unknown')

* balance: bank balance (numeric)

* housing: has housing loan? (categorical: 'no','yes','unknown')

* loan: has personal loan? (categorical: 'no','yes','unknown')

# related with the last contact of the current campaign:
* contact: contact communication type (categorical: 'cellular','telephone')
* month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
* day: last contact day of the week (numeric)
* duration: last contact duration, in seconds (numeric).

# other attributes:

* campaign: number of contacts performed during this campaign and for this client (numeric)

* pdays: number of days that passed by after the client was last contacted from a previous campaign. (numeric)

* previous: number of contacts performed before this campaign and for this client (numeric)

* poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')

Output variable (desired target):

* y has the client subscribed a term deposit? (binary: 'yes','no')

At first let's load the data: 
```{r}
data = read.csv("bank.csv", sep = ";", header = TRUE)
```

```{r}
head(data)
```

Since the data matrix contains one observation for each customer, we identify each individual with a progressive integer ID:

```{r}
data$ID <- seq.int(nrow(data))
```

```{r}
head(data)
```

We can reorder columns in order to have the id column as the first one:

```{r}
data <- data %>%
  select(ID, everything())
```

```{r}
head(data)
```

Check dimension:

```{r}
dim(data)
```

The dataset was correctly read: it has 4521 rows/observations and 18 columns/variables

Check the structure:

```{r}
str(data)
```
As we can see, all categorical variables has been read as 'character' by R, so let's transform them into the 'factor' R data type:

```{r}
data[,"job"] = as.factor(data[,"job"])
data[,"marital"] = as.factor(data[,"marital"])
data[,"education"] = as.factor(data[,"education"])
data[,"default"] = as.factor(data[,"default"])
data[,"housing"] = as.factor(data[,"housing"])
data[,"loan"] = as.factor(data[,"loan"])
data[,"contact"] = as.factor(data[,"contact"])
data[,"month"] = as.factor(data[,"month"])
data[,"poutcome"] = as.factor(data[,"poutcome"])
data[,"y"] = as.factor(data[,"y"])
```

We can check again the structure:

```{r}
str(data)
```

As we can see, the categorical variables have been all correctly converted into the 'factor' type.
We can print a portion (a sample of 20) of the table using kable and the pipe operator, to better understand the structure of the variables:

```{r}
data %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Marketing Campaigns of Portuguese Bank (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Let's now visualize some basic statistics on each of the data frame's columns with *summary*:

```{r}
data %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Marketing Campaigns of Portuguese Bank") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
The previous output allows to understand some important properties of the dataset. At first we can see the possible values assumed by each categorical variable.

Some relevant observations that can be drawn are:

* the dataset represent customers with a medium age of 41 years, which is a pretty realistic scenario in a bank database.

* the 'marital' variable shows that the main customers of the bank are member of families, since most of the 'marital' variable values are in the 'married' category.

* the variable 'balance' assumes also negative values, indicating people with a negative balance in their bank account.

* variables default is highly unbalanced towards the value 'yes', meaning that very few customers have their credit in default.

* the variable 'loan' tells us that most of the bank clients don't have any kind of personal loan.

* the variable 'previous' is characterized by a very low mean value, meaning that the majority of the clients have been contacted only a few times before this specific campaign.

* a value of '-1' in the 'pdays' variable represents clients who have never been contacted before for marketing campaign.

* most of the values of the variable 'poutcome' are 'unknown' meaning that the bank doesn't have any data regarding the outcome of previous campaigns for that client. It could be interesting to understand whether the 'unknown' values are associated only to '-1' values of the 'pdays' variable: if it is the case it means that the bank has no information only about customers who have never been contacted before for a campaign.

* for what regard the target variable 'y', we can see how most of the observations belong to the 'no' class, with only 521 observations belonging to the class 'yes'. This is an important characteristic that we should strongly take into consideration when developing our classification models, in order to avoid giving too much importance to the 'no' class. More observation about this unbalancing issue will be discussed in the following sections.

### Check for missing values

Let's check for NA values for each column:

```{r}
colSums((is.na((data))))
```

There are no missing values!


### Check variables distribution

Before dealing with outliers, we would like to better inspect the data at our disposal by directly visualizing the distribution of each variable. This will allow us to understand the characteristics of them and to decide the strategy to adopt for dealing with eventual outliers.

At first, let's visualize the distribution of each qualitative variable, by using bar plots.
Specifically, these plots are drawn taking into consideration the value of the 'y' variable. In this way we can already start to understand just by a visual inspection, the percentage of subscription to the term deposit, for the individuals in the data set, with respect to the value assumed by the variable under inspection.

'job' variable:

```{r message=FALSE,warning=FALSE}

p <- ggplot(data, aes(x=job, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "job variable",
    x = "job",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```
Is already evident how for some specific type of occupations, the percentage of adhesion to the term deposit is greater than for other kind of jobs.
For example, the individuals performing blue-collar or management jobs are less likely to adhere to the term deposit than for example retired people.


'marital' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=marital, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "marital variable",
    x = "marital",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```


'education' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=education, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "education variable",
    x = "education",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```

'default' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=default, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "default variable",
    x = "default",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

'housing' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=housing, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "housing variable",
    x = "housing",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```
Observations:
From the plot it can be observed how people having no housing loan are more prone to subscribe for the term deposit.
This observation is coherent given that people who are already under a debt condition, needs more liquidity in the short term and cannot afford to keep it untouched in some kind of deposit instrument.


'loan' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=loan, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "loan variable",
    x = "loan",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+
  theme_grey()

p + coord_flip()
```

'contact' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=contact, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "contact variable",
    x = "contact",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

'month' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=month, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "month variable",
    x = "month",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

Observations:
Most of the last contacts have happened in the month of may, or more generally, during the summer months, which may indicate also the starting period of the marketing campaign.
Interestingly, seems like people contacted in the month immediately after the months described above, have an higher possibility of signing for a term deposit.


'poutcome' variable:

```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=poutcome, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "poutcome variable",
    x = "poutcome",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

Observations:
As expected, people who decided to sign for a term deposit in a previous campaign are very prone to re-sign for the current campaign, however, most of the values are 'unknown', meaning that the outcome of the campaign for each client, weren't tracked appropriately by the bank.
One thing who should be highlighted, is that there is some ambiguity on the meaning of 'other' and 'unknown', specifically, the value 'other' seems to be not possible to be interpreted given the information provided, therefore we may decide to convert into 'unknown'.

```{r}
data$poutcome[data$poutcome=="other"] <- "unknown"
```



```{r message=FALSE,warning=FALSE}
p <- ggplot(data, aes(x=poutcome, fill=y))+
  geom_bar(stat="count", width=0.9 )+
  labs(
    title = "poutcome variable",
    x = "poutcome",
    y = "count"
  )+
  scale_fill_brewer(palette="Paired")+ 
  theme_grey()

p + coord_flip()
```

We can now visualize more in detail the distribution and characteristics of the quantitative variables of our dataset.
Let's start by analyzing the distributions:

```{r}
g1 <- ggplot(data, aes(x=age)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g2 <- ggplot(data, aes(x=balance)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g3 <- ggplot(data, aes(x=day)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g4 <- ggplot(data, aes(x=duration)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g5 <- ggplot(data, aes(x=campaign)) +
  geom_histogram(alpha=0.6, fill="#69b3a2", color="#e9ecef", alpha=0.8)
g6 <- ggplot(data, aes(x=pdays)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
g7 <- ggplot(data, aes(x=previous)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)

grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7, nrow=1);
```
The main observations we extracted by a visual inspection of the histograms are the following:

* Most of the clients called by the bank dispose of a bank account with a balance oscillating between 0 to 5000.

* Most of the calls made are done between the second and third week of the month.

* The majority of the calls performed have a really short duration (in second). This may be a crucial variable for understanding the outcome of the campaign for a certain client.

* The majority of the clients are contacted for the first time during this campaign, as can be observed from the distribution of the 'pdays' variable (mostly assumes 0 value).


We can visualize also densities depending on the target variable value:

```{r}
g1 <- ggplot(data, aes(x=age, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="right")
g2 <- ggplot(data, aes(x=balance, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g3 <- ggplot(data, aes(x=day, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g4 <- ggplot(data, aes(x=duration, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g5 <- ggplot(data, aes(x=campaign, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g6 <- ggplot(data, aes(x=pdays, fill=y)) +
   geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")
g7 <- ggplot(data, aes(x=previous, fill=y)) +
    geom_histogram(alpha = 0.7) + theme_grey() +
    theme(legend.position="none")

grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7, nrow=1);
```

Observations:

* As specified before, 'duration' variable has a pretty accentuated role in discriminating between the 2 classes of the target variable, the more the call is long, the more likely is for the client to subscribe in the term deposit.

* 'campaign' distribution is characterized for some small peaks for high values of the variable in corresponding to the class 'no', meaning that the more the client is called, the less it's probable that he will be likely to sign for the term deposit.

We can now get a rough estimate of the distribution of the values for each continuous attribute broken down by each class:

```{r}
library(gridExtra)
g1 <- ggplot(data,aes(x=y, y=age, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="right")
g2 <- ggplot(data,aes(x=y, y=balance, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g3 <- ggplot(data,aes(x=y, y=day, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g4 <- ggplot(data,aes(x=y, y=duration, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g5 <- ggplot(data,aes(x=y, y=campaign, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g6 <- ggplot(data,aes(x=y, y=pdays, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
g7 <- ggplot(data,aes(x=y, y=previous, fill=y)) + 
    geom_boxplot() +
    theme(legend.position="none")
grid.arrange(g1,g2,nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7,nrow=1);

```

Observations:

* For the 'balance' variable we can say that we have a very sparse solution: most of the clients have a very low bank account balance, while we have 2 observations with negative value and other 2 observations with a very high balance value.

* For what regards the 'duration' variable we have 3 samples with very high values of the variable. The distribution of the variable is left skewed, with a very low mean, and there are a lot of values above the interquantile range, which we consider significant for the classification task and we will not consider all of them as outliers.

* Also in 'campaign' we have some observations with too high value: it seems that there are clients that have been contacted more than 40 times, with respect to an average of around 2.8.

We think it is interesting to see the results for the clients with the highest values of 'balance':

```{r}
arr = c(order(data$balance, decreasing=TRUE)[1:10])
data[arr,]
```
We can see that no one of these clients decided to sign for the term deposit.

We can use the *boxplot* function for selecting the outliers:

```{r}
#grab the outliers
outliers = boxplot(data$balance, plot=FALSE)$out

#Extract the outliers from the original data frame
data[data$balance %in% outliers,]
```

The boxplot function selects 506 observations as outliers!
We' ll remove only the first 2 observations with the highest balance:

```{r}
arr = c(order(data$balance, decreasing=TRUE)[1:2])
data = data[-arr, ]
```

Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=balance, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=balance)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

We think also that the variable 'campaign' may be a good indicator for predicting the target variable, since it represents the number of times a client has been contacted during the actual marketing campaign:

```{r}
arr = c(order(data$campaign, decreasing=TRUE)[1:10])
data[arr,]
```
`

What we can understand from the above is that  calling many times a client has not a positive effect on their willingness to sign for the term deposit.
We' ll remove only the first 2 observations with the highest campaign, since they consider clients called more than 40 times, which is a really high value that cannot be considered much realistic, especially considering the mean of that variable (2.8):

```{r}
arr = c(order(data$campaign, decreasing=TRUE)[1:2])
data = data[-arr, ]
```

Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=campaign, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=campaign)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

The same reasoning can be applied for the 'duration' variable:
```{r}
arr = c(order(data$duration, decreasing=TRUE)[1:20])
data[arr,]
```

As pointed out several times before, we thing that this variable is crucial for discriminating between the 2 classes: indeed an higher call duration indicates an higher probability for the client to apply for the term deposit.
We'll remove the first 3 observations with highest value of 'duration':

```{r}
arr = c(order(data$duration, decreasing=TRUE)[1:3])
data = data[-arr, ]
```

Let's see how the distribution for that variables changes:

```{r}
ggplot(data,aes(x=y, y=duration, fill=y)) + 
    geom_boxplot()
```

```{r}
 ggplot(data, aes(x=duration)) +
    geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

In could be interesting to visualize the correlations among these continuous variables:

```{r}
cols = c("age", "balance", "day", "duration", "campaign", "pdays", "previous")
library(GGally)
ggpairs(data, columns=cols,
        ggplot2::aes(colour=y),
        title="Correlation matrix")
```

We have very low correlation values, except for the pair of variables 'pdays' and 'previous'.


## Insights about the data

With the obtained knowledge of the distribution of the variables, we can perform further analysis for each customer characteristics in order to investigate its influence on the subscription rate, i.e. on the rate of subscriptions to the term deposit.

Let's better visualize the scatter plot of the 2 variables 'campaign' and 'duration', encoding the target class in the plot:

```{r}
ggplot(data, aes(x=duration, y=campaign, shape=y, color=y, size=y)) +
  geom_point()
```

As shown in the above plot, 'duration' and 'campaign' variables allow to create approximately 2 clusters of customers:

* almost all the 'yes' clients are characterized by low value of 'campaign' and they may have also high 'duration'.

* 'no' clients have higher values on campaign and low 'duration' values.

An important observation is that an high value of the 'campaign' variable makes the clients to refuse to sign for the term deposit: almost all observations with 'campaign' value above 10 have 'no' as target variable value.


It may be useful to better visualize the scatter plot of the 2 variables 'age' and 'contact', because it is more probable that older people are more used to be contacted by telephone rather than cellular:

```{r}
ggplot(data, aes(x=contact, y=age, shape=y, color=y, size=y)) +
  geom_point()
```

It seems the there is no relationship between 'contact' and 'age'.


Another possible observation could be extracted from the scatter plot of the 2 variables 'age' and 'balance':

```{r}
ggplot(data, aes(x=age, y=balance)) +
  geom_point()
```

The above scatter plot doesn't show any strong relationship among the 2 variables: most of the customers contacted by the bank has low salary, independently from the age.


We can now represent the correlation matrix among these quantitative variables: 
```{r}
# compute correlation matrix
res <- cor(data[, cols])
round(res, 2)
```

```{r}
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
The results confirms what we observed before: only 'pdays' and 'previous' are positively correlated to each other.

### Subscription rate per age
```{r}
data2 = data

data2$age = cut(data2$age, breaks = c(0, 29, 39, 49, 59, 69, 100),
                 labels = c('<30','30-39','40-49',
                            '50-59','60-69','70+'
                            ))
```

```{r}
data$age[16]
```


```{r}
data2 %>% 
    count(age = factor(age), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```

The above plot suggests that:

* clients with a age above 60 have the highest subscription rate: since the percentage of 'yes' is very close to the one of 'no', meaning that a lot of the people contacted by the bank signed for the term deposit.

* it is not surprising to see such a pattern because the main investment objective of older people is saving for retirement while the middle-aged group tend to be more aggressive with a main objective of generating high investment income. Term deposits, as the least risky investment tool, are more preferable to the eldest.

* The youngest may not have enough money or professional knowledge to engage in sophisticated investments, such as stocks and mutual funds. Term deposits provide liquidity and generate interest incomes that are higher than the regular saving account, so term deposits are ideal investments for students.

We could better visualize this characteristics by taking into account only customers that signed a term deposit:


```{r}
data_yes <- data2[which(data2$y == 'yes'), ]  # all yes's of target class

data_yes %>% 
    count(age = factor(age), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, label = scales::percent(pct))) + 
    geom_col(position = 'dodge', fill="#69b3a2", color="#e9ecef", alpha=0.8) + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```

The plot clearly shows that the age range of people most contacted by the bank is the one between 30 and 60, this explains the high percentage of subscriptions. But this may be misleading, as we saw above that older people are much more likely to sign.


### Subscription rate per job

```{r}
data %>% 
    count(job = factor(job), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = job, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)+
    theme_grey()+
    coord_flip()
```

The plot above allows to draw other interesting conclusions, also related to what we saw before:

* the percentage of 'yes' is very high for 'retired' and 'student' categories, as we already found out above when studying the age ranges.


### Subscription rate by balance level

In order to better understand the effect of the balance on the probability to sign for a term contract, we can visualize it by means of an histogram:


```{r}
# cut balances into 
data2 <- data
data2$balance = cut(data2$balance, breaks = c(-5000, -1,  2000, 5000, 10000, 15000, 30000),
                 labels = c('<0', '0-2000', '2000-5000','50000-10000', '10000-15000', '>150000'
                            ))
```


```{r}
data2 %>% 
    count(balance = factor(balance), dep = factor(y)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = balance, y = pct, fill = dep, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)
```
Observations:

* Clients with a balance in the range 2000-5000 are the ones with a highest subscription rate

In order to confirm our hypotheses, we can combine together the information from the ages with that of the balances:

```{r}
data2 <- data
data2$balance = cut(data2$balance, breaks = c(-5000, -1,  5000, 15000, 30000),
                 labels = c('<0', '0-5000', '5000-15000','>15000'
                            ))
data2$age = cut(data2$age, breaks = c(0, 29, 39, 49, 59, 69, 100),
                 labels = c('<30','30-39','40-49',
                            '50-59','60-69','70+'
                            ))
data_yes <- data2[which(data2$y == 'yes'), ]  # all yes's of target class
data_yes %>% 
    count(age = factor(age), bal = factor(balance)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = age, y = pct, fill = bal, label = scales::percent(pct))) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 3) + 
    scale_y_continuous(labels = scales::percent)+
    coord_flip()
```


??????????????????????????????????


We can see that the highest number of 'yes' is in the age range '30-39' for clients with a balance between '0-5000', this means young clients with a sufficient balance are the most prone to sign for the term deposit.


## Preparing the dataset

We'll now prepare the dataset for the classification task. We remind that our goal is to use the customers characteristics to predict whether they will sign for a term deposit. This will allow the bank to identify the customers to contact in future marketing campaigns.

### Feature engineering

We think that the variables 'contact' and 'day' are not very useful for the classification task:

* for what regards the 'day' variable, we have seen in the previous plots how it's density is pretty uniform along the days of the month, therefore we'll not consider this variable for performing the classification algorithms.

* the variable 'contact' indicates the type of mean used to contact the customers: the above plots show that it is not an interesting feature for the classification task.


```{r}
drop = c("contact", "day")
data = data[,!(names(data) %in% drop)]
```

```{r}
str(data)
```

Since the variable 'pdays' is very spread in the past and has a lot of '-1' values (which makes the classification task harder compromising the training), we decided to group values into 7 bins:
```{r}
data$pdays = cut(data$pdays, breaks = c(-2,-1,90,181,272,363,545,max(data$pdays)+1),
                 labels = c('Never contacted','Under 3months','Under 6months',
                            'Under 9months','Under 1yr','Under 1.5yrs',
                            'Over 1.5yrs'))
summary(data$pdays)
```


```{r}
save(data, file="preprocessed_data.RData")
```


### Convert categorical variables

Since most classification algorithms work with continuous variables, we need to one-hot encode categorial variables.
In order to do that, we'll convert all the categorical variables into 'dummy' variables, i.e. we  represent a categorical variable as a numerical variable for each category that takes on one of two values: zero or one.

We'll use the _dummyCols_ function of the *fastDummes* package of R:

Let's see how it works for a single variable ('poutcome'):

```{r}
#install.packages('fastDummies')
library('fastDummies')

# Create dummy variables:
data_dummy <- dummy_cols(data, select_columns = 'poutcome', remove_selected_columns = TRUE)
```

```{r}
head(data_dummy)
```

```{r}
summary(data_dummy)
```

As we can see, the creation of the dummy variables created a variable also for the value 'other' of the 'poutcome' variable, which we converted before into the 'unknown' value. Indeed the new variable 'poutcome_other' has value 0 for all individuals, we'll therefore drop it from our dataframe:

```{r}
drop = c("poutcome_other")
data_dummy = data_dummy[,!(names(data_dummy) %in% drop)]
```

```{r}
str(data_dummy)
```

We can now follow the same reasoning for the other categorical variables:

```{r}
data_dummy <- dummy_cols(data_dummy, select_columns = c('job','marital', 'education', 'default', 'housing', 'loan', 'month', 'pdays'), remove_selected_columns = TRUE)
```

Let's visualize a sample of the newly created data matrix:


```{r}
data_dummy %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Marketing Campaigns of Portuguese Bank (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


The last operation we need to do is to convert the 'yes' value of 'y' variable into 1 and 'no' into 0:
```{r}
data_dummy$y <- ifelse(data_dummy$y=="yes",1,0)
```

```{r}
data_dummy %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Marketing Campaigns of Portuguese Bank") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

### Multivariate outliers

Now that we have the explored enough the data and converted it into the final format, we can perform some multivariate outliers detection algorithms, for detecting and removing outliers in the multivariate space.

One way to check for multivariate outliers  (non-parametric approach) is to use the LOF ("local outlier factor") algorithm, which identifies density-based local outliers.

The algorithm we are going to use (from the package [DDoutlier](https://rdrr.io/cran/DDoutlier/man/LOF.html)) computes a local density for observations with a given k-nearest neighbors (we choose k = 5). This local density is compared to the density of the respective nearest neighbors, resulting in the local outlier factor.

Therefore, the function returns a vector of LOF scores for each observation: the greater the LOF, the greater the outlierness of the data point.


```{r message=FALSE, warning=FALSE}
#install.packages('DDoutlier')
library("DDoutlier")
lof <- LOF(data_dummy, k = 5) # outlier score with a neighborhood of 5 points
```

We can show the lof scores for the 5 first observations:

```{r}
head(lof)
```

We can see and visualize the distribution of outlier scores:

```{r}
summary(lof) # some statistics
hist(lof)
```

It could be useful to plot also the sorted LOF scores:

```{r}
plot(sort(lof), type = "l",  main = "LOF (K = 5)",
  xlab = "Points sorted by LOF", ylab = "LOF")
```

Looks like outliers start around a LOF value of 2.0.

Let's show the indexes for 5 most outlying observations:

```{r}
lof_with_names = lof
names(lof_with_names) <- 1:nrow(data_dummy)
sort(lof_with_names, decreasing = TRUE)[1:5]
```

Indexes of the outliers with a lof score above 2.0:

```{r}
outliers_lof <- which(lof > 2.0)
```

Number of detected outliers:

```{r}
length(outliers_lof)
```

```{r}
outliers_lof
```

We will simply remove the found outliers (found with lof) from the dataset (taking into account that algorithms such as k-means are very sensitive to outliers):

```{r}
data_dummy = data_dummy[-outliers_lof,] 
```

Let's now check again the dimensions:

```{r}
dim(data_dummy)
```

The outliers have been correctly removed!


### PCA

A further step to gain more insights about the dataset could be to perform dimensionality reduction by applying factor analysis methods.

Since we don't have specific groups of variables, we'll perform first a PCA by using the continuous features and by adding the categorical variables as supplementary information:

* our purpose is to find some useful relationships between variables and individuals, with respect to the class

* we will exploit these insights to perform classification

We'll use *FactoMineR* for performing the PCA:

```{r}
str(data_dummy)
```


```{r}
data_ = data_dummy[2:18]
res.pca = PCA(data_, scale.unit=TRUE, ncp=5, graph=F)
```

```{r}
fviz_eig(res.pca)
```

```{r}
?fviz_pca_ind
```


Individuals plot:

```{r}
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             select.ind = list(contrib =50),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
fviz_pca_ind(res.pca,
             col.ind = data_dummy$y,
             repel = TRUE     # Avoid text overlapping
             )
```


We are more interested in visualizing the variables plot:

```{r}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
LE VARIABILI SCRITTE SOTTO NON COMPAIONO NEL PLOT ??????????????????????????????

The above PCA plot shows an high correlation for the variables 'previous' and 'pdays' as well as for the variables 'campaign' and 'day'. Variable 'duration' is negatively correlated with 'campaign' and 'day'. Jobs related to an high income like 'job_management' and 'job_admin' are as well correlated.
The same can be said for 'job_retired' and 'age', the more the age is, the more likely the client will have retired from the job market. Variables balance and age are not well represented.

We may perform the plot on the 3rd and 4th PCs:

```{r}
fviz_pca_var(res.pca,
             axes = c(3,4),
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
ANCHE QUA SEMBRA CHE LE VARIABILI NON SIANO COSI' TANTO RELAZIONATE??????????

We can see that on the 2nd factorial plane the variables 'balance' and 'age' are very correlated with each other, and they are very correlated with the 3rd dimension. 
The correlation between 'balance' and 'age' may be easily interpretable with the fact that younger people are unlikely to have high balances in their bank account.

Biplot:

```{r}
data_dummy[,"y"] = as.factor(data_dummy[,"y"])
fviz_pca_biplot(res.pca, repel = TRUE,
                axes = c(1,2),
                col.var = "black", # Variables color
                col.ind = data_dummy$y,
                addEllipses = TRUE
                )
```

Since PCA tries to maximize the variance inside the dataset, it is not so useful for a classification task.
Our next step is therefore to try to apply an LDA, just for visualizing better the data, specifically, we tried to understand if the two classes could be linearly separated:

```{r}
library(MASS)
(model <- lda(y~., data = data_dummy))
```

```{r}
plot(model)
```

?????? CONCLUSIONI???????


### Splitting

We' ll split the dataset into training and validation:

* 80% of the original data will be used for training

* the reaming 20% will be used to evaluate the final performance of the classifier

An important remark is that the considered dataset is highly unbalanced: most of the customers belong to the 'no' values of the target variable. For that reason, and in order to have a fair evaluation of our algorithms we'll perform a stratified splitting:

* we'll select 80% among all the clients belonging to the positive class for training and 20% for testing

* we'll select 80% among all the clients belonging to the negative class for training and 20% for testing

In this way we'll have the same distribution of observations inside the two classes in both the training and test set.


```{r}
data_dummy <- data_dummy %>% relocate(y, .after = "pdays_Over 1.5yrs")
```

```{r}
str(data_dummy)
```


```{r}
set.seed(123)
#Training & Test Datasets
deposit_yes <- data_dummy[which(data_dummy$y == 1), ]  # all yes's of target class
deposit_no <- data_dummy[which(data_dummy$y ==  0), ]  # all no's of target class

deposit_yes_training_rows <- sample(1:nrow(deposit_yes), 0.8*nrow(deposit_yes))  #randomly choosing 80% observations of yes class
deposit_no_training_rows <- sample(1:nrow(deposit_no), 0.8*nrow(deposit_no))  #randomly choosing 80% observations of no class
training_yes <- deposit_yes[deposit_yes_training_rows, ]  
training_no <- deposit_no[deposit_no_training_rows, ]
trainingData <- rbind(training_yes, training_no)  #combining chosen observations
glimpse(trainingData)
table(trainingData$y)

```


In the training data we have 3185 samples in the 'no' class and 412 samples in the 'yes' class.


```{r}
# Create Test Data
test_yes <- deposit_yes[-deposit_yes_training_rows, ]
test_no <- deposit_no[-deposit_no_training_rows, ]
testData <- rbind(test_yes, test_no)  #combining chosen observations
glimpse(testData)
table(testData$y)
```

In the test data we have 797 samples in the 'no' class and 104 samples in the 'yes' class.

We can save the training and test dataset, so that we can direclty use them in the main algorithms:

```{r}
#save(trainingData, testData, trainingData_scaled, testData_scaled, file = "data.RData")
save(trainingData, testData, file = "data.RData")
```


